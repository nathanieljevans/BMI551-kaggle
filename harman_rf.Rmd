---
title: "Final Project: BMI 651"
author: "Gareth Harman"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---




```{r, include = F, warning=F, results='hide', echo=F}

library(tidyverse) # all the good stuff
library(readxl) # for reading in xlsx files
library(janitor) # for clean_names
library(knitr) # for kable
library(moderndive) # for getting tables
library(skimr) # for skim
library(broom) # for pulling out model results
library(dplyr)
library(randomForest)

select <- dplyr::select
```

# Load Data

```{r}

exp.dat <- read.csv2('./data/expression.txt', header=T, sep='\t', numerals='warn.loss', stringsAsFactors = F)
rnd.sub.cnt <- read.csv2('./data/rand_sub_cont.csv', sep=',')
id.map <- read.csv2('./data/scoring_and_test_set_id_mappings.csv', sep=',')
sub.types <- read.csv2('./data/subtypes.txt', sep='\t')
train.dat <- read.csv2('./data/training_set_answers.txt', sep='\t')

gene.names <- rownames(exp.dat)
line.names <- colnames(exp.dat)
rownames(exp.dat) <- NULL
exp.dat <- mutate_all((exp.dat), function(x) as.numeric(x))
colnames(exp.dat) <- NULL
exp.dat <- t(exp.dat)
colnames(exp.dat) <- gene.names
exp.dat <- cbind(data.frame('cell_line'=line.names), exp.dat)

# FIX BAD COLUMN NAMES!!!
colnames(exp.dat) <- paste0('x_', colnames(exp.dat))
colnames(exp.dat) <- gsub('.', '_', colnames(exp.dat), fixed=T)
colnames(exp.dat) <- gsub('-', '_', colnames(exp.dat), fixed=T)
```

# Variance feature selection (required for RF)

```{r}

# Add feature name and variance value
variance_values <- c()
variance_names <- c()

for (ii in names(exp.dat)){
  variance_values[length(variance_values) + 1] <- var(exp.dat[ii])
  variance_names[length(variance_names) + 1] <- ii
}

# Create a dataframe of variance
varDF <- data.frame(name = variance_names, var = variance_values)
varSort <- sort(variance_values)

# Return only above .5 (~3732 features)
varDF <- varDF %>% filter(var >= .5)
var_names <- as.vector(varDF$name)

# Create a new dataframe from these filtered values
exp.filt <- select(exp.dat, one_of(var_names), x_cell_line)

```

# Create the feature set

```{r}

# Edit features of the response names
colnames(train.dat) <- paste0("out_", colnames(train.dat))
colnames(train.dat) <- gsub('.', '_', colnames(train.dat), fixed = T)
train.dat$x_cell_line <- rownames(train.dat)

# Merge training data into one feature data.frame
train.full <- merge(exp.filt, train.dat, by = "x_cell_line")

# Create a onehot encoded version of subtype
sub_type <- sub.types %>%
  mutate(type_luminal = ifelse(subtype == "Luminal", 1, 0)) %>%
  mutate(type_basal = ifelse(subtype == "Basal", 1, 0)) %>%
  mutate(type_claudin = ifelse(subtype == "Claudin-low", 1, 0)) %>%
  mutate(type_normal = ifelse(subtype == "Normal-like", 1, 0)) %>%
  select(-subtype) 

# Add cell_line to feature set
colnames(sub_type)[1] <- "x_cell_line"

# Get subtype only of those in the training data
sub_type_train <- sub_type %>%
  filter(x_cell_line %in% train.full$x_cell_line)

# Add the subtype to training data
train.full <- merge(train.full, sub_type_train, by = "x_cell_line")

# Fix some column name issues
colnames(train.full) <- gsub(".", "_", colnames(train.full), fixed = T)

```

# Setup test data

```{r}

# Get subtype only of those in the training data
sub_type_test <- sub_type %>%
  filter(!x_cell_line %in% train.full$x_cell_line)

# Fix the stupid cell line naming issue
sub_type_test$x_cell_line <- as.vector(sub_type_test$x_cell_line)
sub_type_test[1, 1] = 'X600MPE'

# Create test data
test.full.inc <- merge(exp.filt, sub_type_test, by = "x_cell_line")

```

# Run the RF

```{r}

# Create list of response drug names 
name.frame <- data.frame(names = names(train.dat))
name.frame$names <- gsub('.', '-', name.frame$names, fixed = T)

# Create the training data without response data
run.df <- select(train.full, -one_of(as.vector(name.frame$names)))

# Set mtry as sqrt(p) 
mtry.val <- round(sqrt(length(names(run.df))))

# Vector of outcome names to keep adding to the run.df
resp.names <- name.frame$names[-length(name.frame$names)] # All but celline

```

```{r}

# Setup dataframe for final output
final.pred.raw <- data.frame(cell_line = test.full.inc$x_cell_line)
final.pred.refine <- data.frame(cell_line = test.full.inc$x_cell_line)

# Test data sans cellline
test.full <- test.full.inc %>% select(-x_cell_line)

# Update overall accuracy
update.raw <- c()
update.ref <- c()

for (ii in resp.names){ 

  print(paste('Running:', ii))
  
  # Append the current outcome 
  fact.val <- as.factor(ifelse(train.full[ii] == 1, 'responder', 'not'))
  run.df$y <- fact.val 
  
  # Run the RF on all features
  rf.all = randomForest(data = run.df,
                               y ~ ., 
                               ntree = 5000,  
                               mtry=mtry.val, 
                               importance=TRUE,
                               maxnodes = 10
                               )
  
  # Get accuracy from the model on the TRAINING DATA
  acc <- sum(rf.all$predicted == run.df$y)/length(run.df$y)
  print(paste('Raw Accuracy', acc))
  update.raw[length(update.raw)+1] <- acc
  
  # Predicted raw
  pred.run.raw <- predict(rf.all, test.full)
  pred.val.raw <- ifelse(pred.run.raw == 'responder', 1, 0)
  final.pred.raw[ii] <- pred.val.raw
  
  #############################################
  #RF FEATURE SELECTION
  #############################################
  
  # Dataframe of variable and importance
  rf.feat <- data.frame(name = rownames(rf.all$importance),
                        imp = rf.all$importance[, 3])
  
  # Sort by variable importance
  imp.sort <- sort(rf.feat$imp)
  
  n.rf.feat <- 100 # Number of features to grab
  
  # Return only the top 10 features 
  rf.feat <- rf.feat %>% filter(imp > imp.sort[length(imp.sort) - n.rf.feat])  
  rf.feat.names <- as.vector(rf.feat$name)
  
  # Create the new dataframe with subtype, response, and top 10 features 
  refine.df <- run.df %>% select(y, type_basal, type_luminal, type_normal, type_claudin,
                                 one_of(rf.feat.names))
  
  # Need an updated mtry.val
  mtry.val.refine <- round(sqrt(length(names(refine.df))))

  # Rerun the RF
  rf.refine= randomForest(data = refine.df,
                             y ~ ., 
                             ntree = 2500,  
                             mtry=mtry.val.refine, 
                             importance=TRUE
                             #maxnodes = 10
                             )
  
  # Recompute accuracy
  acc <- sum(rf.refine$predicted == run.df$y)/length(run.df$y)
  print(paste('Refined Accuracy', acc))
  update.ref[length(update.ref)+1] <- acc

  # Add to prediction data.frame
  pred.run.refine <- predict(rf.refine, test.full)
  pred.val.refine <- ifelse(pred.run.refine == 'responder', 1, 0)
  final.pred.refine[ii] <- pred.val.refine
  
  # Remove current treatment 
  run.df <- run.df %>% select(-y)
}

print(paste('Mean Raw:', mean(update.raw)))
print(paste('Mean Ref:', mean(update.ref)))
```

### Last specs
- RF Filt: 5 
- Tree: 2500
- Var Filter: >= .5
- Mean RAW: .64
- Mean REF: .90

# Format

```{r}

id.cellline <- as.character(id.map$cellline)

for (i in 1:168) { 
  if (id.cellline[i] %in% '600MPE') { 
    id.cellline[i] <- 'X600MPE'  
  }
}

id.drug <- as.character(paste0('out_', id.map$drug))
id.drug <- gsub('-', '_', id.drug, fixed=T)

final.write.raw <- c()
final.write.ref <- c()

for (ii in 1:168) {
  
  cell.ind.raw <- which(final.pred.raw$cell_line == id.cellline[ii])
  cell.ind.ref <- which(final.pred.refine$cell_line == id.cellline[ii])
  
  temp.raw.vect <- final.pred.raw[id.drug[ii]]
  temp.ref.vect <- final.pred.refine[id.drug[ii]]
  
  final.write.raw[length(final.write.raw)+1] <- temp.raw.vect[cell.ind.raw, 1]
  final.write.ref[length(final.write.ref)+1] <- temp.ref.vect[cell.ind.ref, 1]
  
}

df.final.raw <- data.frame(id = seq(1, 168), value = final.write.raw)
df.final.ref <- data.frame(id = seq(1, 168), value = final.write.ref)

write.table(df.final.raw, file='./harman_rf_raw.csv', sep=',', row.names = F)
write.table(df.final.ref, file='./harman_rf_ref.csv', sep=',', row.names = F)
```

```



# RF tuning (not necessary)
```{r}

Xtrain <- run.df %>% select(-y)
Ytrain <- train.full[[ii]]

tuneRF(Xtrain, Ytrain, mtryStart = 3, 1000, stepFactor = 10, improve = .001, trace = T, plot=T)

```

# PLS-LDA not great ~ 70% ACC
```{r}

Xtrain <- run.df %>% select(-y)
Ytrain <- train.full[[ii]]

pls.mod <- pls.lda(Xtrain, Ytrain, ncomp = 4, nruncv=20)

acc <- 1 - sum(abs((c(pls.mod$predclass)-1) - Ytrain))/length(Ytrain)

```

# Elastic-net horrible
```{r}

lambda.grid <- seq(0, 100)
alpha.grid <- seq(0, 0.5, length = 6)

trnCtrl = trainControl(
             method = "repeatedCV",
             number = 5,
             repeats = 3)

srchGrd = expand.grid(.alpha = alpha.grid, .lambda = lambda.grid)

my.train <- train(x = data.matrix(Xtrain),
                  y = Ytrain,
                  method = "glmnet",
                  tuneGrid = srchGrd,
                  trControl = trnCtrl,
                  standardize = FALSE,
                  maxit = 1000000)

plot(my.train)

```

# Lasso horrible
```{r}

x <- model.matrix(y~.,run.df)[,-1]
y <- ifelse(run.df$y == 'responder', 0, 1)

cv <- cv.glmnet(x, y, alpha = 1)
cv$lambda.min

model <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
coef(model)

x.test <- model.matrix(y~.,run.df)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()

# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, y),
  Rsquare = R2(predictions, y)
)
```

